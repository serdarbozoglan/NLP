{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text) # word or punctuation chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world!"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('It costs $5.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  [It, costs, $, 5, .]\n"
     ]
    }
   ],
   "source": [
    "print('Tokens: ', [token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print('Index: ', [token.i for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_alpha:  [True, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print('is_alpha: ', [token.is_alpha for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_punct:  [False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "print('is_punct: ', [token.is_punct for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like_num:  [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "print('like_num: ', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('It costs $5 and ten cents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like_num:  [False, False, False, True, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "print('like_num: ', [token.like_num for token in doc])\n",
    "# can detect \"TEN\" as a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_token = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('She ate the pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She    ------PRON\n",
      "ate    ------VERB\n",
      "the    -------DET\n",
      "pizza  ------NOUN\n"
     ]
    }
   ],
   "source": [
    "#iterate over the tokens\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{6}} {token.pos_:->{10}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She    ------PRON      nsubj      ate\n",
      "ate    ------VERB       ROOT      ate\n",
      "the    -------DET        det    pizza\n",
      "pizza  ------NOUN       dobj      ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:{6}} {token.pos_:->{10}} {token.dep_:>{10}} {token.head.text:>{8}}\")\n",
    "    # dep_ return dependencies (subject or object)\n",
    "    # token.head.tetx --> parent token of. Shows the child tokens\n",
    "    # nsubj (nominal subject), dobj(direct object), det(determiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entitites\n",
    "\n",
    "doc = nlp(u'Apple is looking at buying U.K. starup for $1 billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like_num:  [False, False, False, False, False, False, False, False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "print('like_num: ', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple      ORG\n",
      "U.K.       GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Iterate overt the predicted entitites\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:{10}} {ent.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monetary values, including unit'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('MONEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NNP')  # also for dependency labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rule-Match \n",
    "\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "## Add the pattern to the macther\n",
    "pattern = [{'ORTH':'iPhone'}, {'ORTH':'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)  # IPHONE_PATTERN is a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"New iPhone X relase date leaked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=[{'IS_DIGIT': True},\n",
    "         {'LOWER':'fifa'},\n",
    "         {'LOWER':'world'},\n",
    "         {'LOWER':'cup'},\n",
    "         {'IS_PUNCT': True}]\n",
    "\n",
    "# Token include digits, punct and case INsensitive fifa, world, cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('2018 FIFA World Cup: France Won!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('WORLDCUP_PATTERN', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "macthes = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIFA World\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LEMMA':'love', 'POS':'VERB'},{'POS':'NOUN'}]\n",
    "\n",
    "# We are looking for a 'Love' verb followed by a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I loved dogs but now I love cats more')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n"
     ]
    }
   ],
   "source": [
    "matcher.add('LOVE_PATTERN', None, pattern)\n",
    "macthes = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 4\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write one pattern that matches adjectives ('ADJ') followed by one or two 'NOUN's \n",
    "# (one noun and one optional noun)\n",
    "\n",
    "\n",
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function remove:\n",
      "\n",
      "remove(...) method of spacy.matcher.matcher.Matcher instance\n",
      "    Remove a rule from the matcher. A KeyError is raised if the key does\n",
      "    not exist.\n",
      "    \n",
      "    key (unicode): The ID of the match rule.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(matcher.remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method-wrapper '__contains__' of spacy.matcher.matcher.Matcher object at 0x1f10b7de0>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.__contains__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 6\n",
      "Match found: radical system\n",
      "Match found: wide redesign\n",
      "Match found: aesthetic upheaval\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "## Write one pattern that only matches mentions of the full iOS versions: \n",
    "# \"iOS 7\", \"iOS 11\" and \"iOS 10\".\n",
    "\n",
    "\n",
    "doc = nlp(\"\"\"After making the iOS update you won't notice a radical system-wide redesign: \n",
    "            nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture \n",
    "            remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\"\"\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'ORTH': 'iOS'}, {'IS_DIGIT': True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_hash = nlp.vocab.strings['coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197928453018144401"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True False\n"
     ]
    }
   ],
   "source": [
    "## LEXEMES\n",
    "\n",
    "doc = nlp('I love coffee')\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "# print the lexical attributes\n",
    "\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha, lexeme.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docs, Span and Entities from sctrach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie \n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, )  # span = Span(doc, 2, 4, label='PERSON') but label does not work \n",
    "print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', '')]\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, )\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    " nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comaring two dcoumnets in terms of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n"
     ]
    }
   ],
   "source": [
    "# to use similarity attribute we are to use either medium or large model of spaCy\n",
    "\n",
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8038315411687904\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp('I like swimming')\n",
    "print(doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491934178104581\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp('I like skjfhsd')\n",
    "print(doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7369546743653412"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare 2 tokens\n",
    "\n",
    "doc = nlp('I like pizza and pasta')\n",
    "token1 = nlp(\"pizza\")\n",
    "token2 = nlp('pasta')\n",
    "\n",
    "token1.similarity(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21193229601905125"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I like pizza and pasta')\n",
    "token1 = nlp(\"bus\")\n",
    "token2 = nlp('dog')\n",
    "\n",
    "token1.similarity(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32531983166759537\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "\n",
    "doc =nlp('I like pizza')\n",
    "token = nlp(\"soap\")[0]\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "\n",
    "span = nlp('I like pizza and pasta')[2:5]\n",
    "doc = nlp('McDonalds sells burgers')\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana\n"
     ]
    }
   ],
   "source": [
    "## Similarity is determined using word vectors\n",
    "## Word2Vec is used\n",
    "\n",
    "doc = nlp('I have a banana')\n",
    "\n",
    "print(doc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "print(doc[3].vector)\n",
    "# multi dimentinal word embediing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", None, [{'LOWER':'golden'}, {\"LOWER\":\"retriever\"}])\n",
    "\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span: ', span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root token:  Retriever\n",
      "Root head token :  have\n",
      "Prevoiuos Token:  a DET\n"
     ]
    }
   ],
   "source": [
    "## Root token\n",
    "\n",
    "print('Root token: ', span.root.text) # Root token is Retriever\n",
    "print('Root head token : ', span.root.head.text) # Root Head Token is Have \n",
    "\n",
    "# Previous token:\n",
    "\n",
    "print('Prevoiuos Token: ', doc[start-1].text, doc[start-1].pos_) # prevoius token is a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "## Phrase Matching is used for key word search in the document\n",
    "## Faster than Matcher\n",
    "## Great for large word lists\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp('Golden Retriever')\n",
    "matcher.add('DOG', None, pattern)\n",
    "\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span: ', span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "list_of_countries = ['Afghanistan',\n",
    " 'Åland Islands',\n",
    " 'Albania',\n",
    " 'Algeria',\n",
    " 'American Samoa',\n",
    " 'Andorra',\n",
    " 'Angola',\n",
    " 'Anguilla',\n",
    " 'Antarctica',\n",
    " 'Antigua and Barbuda',\n",
    " 'Argentina',\n",
    " 'Armenia',\n",
    " 'Aruba',\n",
    " 'Australia',\n",
    " 'Austria',\n",
    " 'Azerbaijan',\n",
    " 'Bahamas',\n",
    " 'Bahrain',\n",
    " 'Bangladesh',\n",
    " 'Barbados',\n",
    " 'Belarus',\n",
    " 'Belgium',\n",
    " 'Belize',\n",
    " 'Benin',\n",
    " 'Bermuda',\n",
    " 'Bhutan',\n",
    " 'Bolivia (Plurinational State of)',\n",
    " 'Bonaire, Sint Eustatius and Saba',\n",
    " 'Bosnia and Herzegovina',\n",
    " 'Botswana',\n",
    " 'Bouvet Island',\n",
    " 'Brazil',\n",
    " 'British Indian Ocean Territory',\n",
    " 'United States Minor Outlying Islands',\n",
    " 'Virgin Islands (British)',\n",
    " 'Virgin Islands (U.S.)',\n",
    " 'Brunei Darussalam',\n",
    " 'Bulgaria',\n",
    " 'Burkina Faso',\n",
    " 'Burundi',\n",
    " 'Cambodia',\n",
    " 'Cameroon',\n",
    " 'Canada',\n",
    " 'Cabo Verde',\n",
    " 'Cayman Islands',\n",
    " 'Central African Republic',\n",
    " 'Chad',\n",
    " 'Chile',\n",
    " 'China',\n",
    " 'Christmas Island',\n",
    " 'Cocos (Keeling) Islands',\n",
    " 'Colombia',\n",
    " 'Comoros',\n",
    " 'Congo',\n",
    " 'Congo (Democratic Republic of the)',\n",
    " 'Cook Islands',\n",
    " 'Costa Rica',\n",
    " 'Croatia',\n",
    " 'Cuba',\n",
    " 'Curaçao',\n",
    " 'Cyprus',\n",
    " 'Czech Republic',\n",
    " 'Denmark',\n",
    " 'Djibouti',\n",
    " 'Dominica',\n",
    " 'Dominican Republic',\n",
    " 'Ecuador',\n",
    " 'Egypt',\n",
    " 'El Salvador',\n",
    " 'Equatorial Guinea',\n",
    " 'Eritrea',\n",
    " 'Estonia',\n",
    " 'Ethiopia',\n",
    " 'Falkland Islands',\n",
    " 'Faroe Islands',\n",
    " 'Fiji',\n",
    " 'Finland',\n",
    " 'France',\n",
    " 'French Guiana',\n",
    " 'French Polynesia',\n",
    " 'French Southern Territories',\n",
    " 'Gabon',\n",
    " 'Gambia',\n",
    " 'Georgia',\n",
    " 'Germany',\n",
    " 'Ghana',\n",
    " 'Gibraltar',\n",
    " 'Greece',\n",
    " 'Greenland',\n",
    " 'Grenada',\n",
    " 'Guadeloupe',\n",
    " 'Guam',\n",
    " 'Guatemala',\n",
    " 'Guernsey',\n",
    " 'Guinea',\n",
    " 'Guinea-Bissau',\n",
    " 'Guyana',\n",
    " 'Haiti',\n",
    " 'Heard Island and McDonald Islands',\n",
    " 'Holy See',\n",
    " 'Honduras',\n",
    " 'Hong Kong',\n",
    " 'Hungary',\n",
    " 'Iceland',\n",
    " 'India',\n",
    " 'Indonesia',\n",
    " \"Côte d'Ivoire\",\n",
    " 'Iran',\n",
    " 'Iraq',\n",
    " 'Ireland',\n",
    " 'Isle of Man',\n",
    " 'Israel',\n",
    " 'Italy',\n",
    " 'Jamaica',\n",
    " 'Japan',\n",
    " 'Jersey',\n",
    " 'Jordan',\n",
    " 'Kazakhstan',\n",
    " 'Kenya',\n",
    " 'Kiribati',\n",
    " 'Kuwait',\n",
    " 'Kyrgyzstan',\n",
    " \"Lao People's Democratic Republic\",\n",
    " 'Latvia',\n",
    " 'Lebanon',\n",
    " 'Lesotho',\n",
    " 'Liberia',\n",
    " 'Libya',\n",
    " 'Liechtenstein',\n",
    " 'Lithuania',\n",
    " 'Luxembourg',\n",
    " 'Macao',\n",
    " 'Macedonia',\n",
    " 'Madagascar',\n",
    " 'Malawi',\n",
    " 'Malaysia',\n",
    " 'Maldives',\n",
    " 'Mali',\n",
    " 'Malta',\n",
    " 'Marshall Islands',\n",
    " 'Martinique',\n",
    " 'Mauritania',\n",
    " 'Mauritius',\n",
    " 'Mayotte',\n",
    " 'Mexico',\n",
    " 'Micronesia',\n",
    " 'Moldova',\n",
    " 'Monaco',\n",
    " 'Mongolia',\n",
    " 'Montenegro',\n",
    " 'Montserrat',\n",
    " 'Morocco',\n",
    " 'Mozambique',\n",
    " 'Myanmar',\n",
    " 'Namibia',\n",
    " 'Nauru',\n",
    " 'Nepal',\n",
    " 'Netherlands',\n",
    " 'New Caledonia',\n",
    " 'New Zealand',\n",
    " 'Nicaragua',\n",
    " 'Niger',\n",
    " 'Nigeria',\n",
    " 'Niue',\n",
    " 'Norfolk Island',\n",
    " \"Korea\",\n",
    " 'Northern Mariana Islands',\n",
    " 'Norway',\n",
    " 'Oman',\n",
    " 'Pakistan',\n",
    " 'Palau',\n",
    " 'Palestine, State of',\n",
    " 'Panama',\n",
    " 'Papua New Guinea',\n",
    " 'Paraguay',\n",
    " 'Peru',\n",
    " 'Philippines',\n",
    " 'Pitcairn',\n",
    " 'Poland',\n",
    " 'Portugal',\n",
    " 'Puerto Rico',\n",
    " 'Qatar',\n",
    " 'Republic of Kosovo',\n",
    " 'Réunion',\n",
    " 'Romania',\n",
    " 'Russian Federation',\n",
    " 'Rwanda',\n",
    " 'Saint Barthélemy',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha',\n",
    " 'Saint Kitts and Nevis',\n",
    " 'Saint Lucia',\n",
    " 'Saint Martin',\n",
    " 'Saint Pierre and Miquelon',\n",
    " 'Saint Vincent and the Grenadines',\n",
    " 'Samoa',\n",
    " 'San Marino',\n",
    " 'Sao Tome and Principe',\n",
    " 'Saudi Arabia',\n",
    " 'Senegal',\n",
    " 'Serbia',\n",
    " 'Seychelles',\n",
    " 'Sierra Leone',\n",
    " 'Singapore',\n",
    " 'Sint Maarten (Dutch part)',\n",
    " 'Slovakia',\n",
    " 'Slovenia',\n",
    " 'Solomon Islands',\n",
    " 'Somalia',\n",
    " 'South Africa',\n",
    " 'South Georgia and the South Sandwich Islands',\n",
    " 'Korea (Republic of)',\n",
    " 'South Sudan',\n",
    " 'Spain',\n",
    " 'Sri Lanka',\n",
    " 'Sudan',\n",
    " 'Suriname',\n",
    " 'Svalbard and Jan Mayen',\n",
    " 'Swaziland',\n",
    " 'Sweden',\n",
    " 'Switzerland',\n",
    " 'Syrian Arab Republic',\n",
    " 'Taiwan',\n",
    " 'Tajikistan',\n",
    " 'Tanzania, United Republic of',\n",
    " 'Thailand',\n",
    " 'Timor-Leste',\n",
    " 'Togo',\n",
    " 'Tokelau',\n",
    " 'Tonga',\n",
    " 'Trinidad and Tobago',\n",
    " 'Tunisia',\n",
    " 'Turkey',\n",
    " 'Turkmenistan',\n",
    " 'Turks and Caicos Islands',\n",
    " 'Tuvalu',\n",
    " 'Uganda',\n",
    " 'Ukraine',\n",
    " 'United Arab Emirates',\n",
    " 'United Kingdom',\n",
    " 'UK',\n",
    " 'United States of America',\n",
    " 'USA',\n",
    " 'U.S.',\n",
    " 'U.S.A',                   \n",
    " 'Uruguay',\n",
    " 'Uzbekistan',\n",
    " 'Vanuatu',\n",
    " 'Venezuela',\n",
    " 'Viet Nam',\n",
    " 'Wallis and Futuna',\n",
    " 'Western Sahara',\n",
    " 'Yemen',\n",
    " 'Zambia',\n",
    " 'Zimbabwe']\n",
    "\n",
    "doc = nlp('Czech Republic may help Slovakia in economy')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(list_of_countries))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING A CUSTOM ENTITY RECOGNIZER/MATCHER (COUNTRY FOR THIS CASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityMatcher(object):\n",
    "    name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp(term) for term in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for label, start, end in matches:\n",
    "            span = Span(doc, start, end, label=label)\n",
    "            spans.append(span)\n",
    "        doc.ents = spans\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'entity_matcher', 'ner']\n"
     ]
    }
   ],
   "source": [
    "entity_matcher = EntityMatcher(nlp, list_of_countries, 'COUNTRY')\n",
    "nlp.add_pipe(entity_matcher, before='ner')\n",
    "#nlp.add_pipe(entity_matcher)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction a specific entity form the text\n",
    "\n",
    "text = \"\"\"After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more\n",
    "missions in ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted\n",
    "Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The \n",
    "UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and \n",
    "oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN\n",
    "authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General\n",
    "from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization,\n",
    "given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression\n",
    "by one nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations\n",
    "such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely viewed as a \n",
    "failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission to Bosnia faced\n",
    "'worldwide ridicule' for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN \n",
    "Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. \n",
    "Beginning in the last decades of the Cold War, American and European critics of the UN condemned the organization for\n",
    "perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew his nation\\'s funding from\n",
    "UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of\n",
    "mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, \n",
    "initiated a reform of the Secretariat, reducing the size of the organization somewhat. His successor, Kofi Annan\n",
    "(1997–2006), initiated further management reforms in the face of threats from the United States to withhold its UN\n",
    "dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. \n",
    "The UN mission in the Sierra Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion\n",
    "of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN \n",
    "Security Council resolution for authorization, prompting a new round of questioning of the organization\\'s \n",
    "effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises\n",
    "including the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers \n",
    "and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final \n",
    "battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". One \n",
    "hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization\\'s history. \n",
    "The Millennium Summit was held in 2000 to discuss the UN\\'s role in the 21st century. The three day meeting was the \n",
    "largest gathering of world leaders in history, and culminated in the adoption by all member states of the Millennium \n",
    "Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, \n",
    "gender equality, and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven.\n",
    "The 2005 World Summit reaffirmed the UN\\'s focus on promoting development, peacekeeping, human rights, and global \n",
    "security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. In \n",
    "addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy \n",
    "by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in \n",
    "2016 the organization held its first public debate between candidates for Secretary-General. On 1 January 2017, \n",
    "Portuguese diplomat António Guterres, who previously served as UN High Commissioner for Refugees, became the ninth \n",
    "Secretary-General. Guterres has highlighted several key goals for his administration, including an emphasis on \n",
    "diplomacy for preventing conflicts, more effective peacekeeping efforts, \n",
    "and streamlining the organization to be more responsive and versatile to global needs. New York, Ankara in May 2000.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"Iraq, Italy, Belgium , apple book came here. In 1990 the USA was perfect but Turkey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia         -->    COUNTRY\n",
      "South Africa    -->    COUNTRY\n",
      "Cambodia        -->    COUNTRY\n",
      "Kuwait          -->    COUNTRY\n",
      "Somalia         -->    COUNTRY\n",
      "Haiti           -->    COUNTRY\n",
      "Mozambique      -->    COUNTRY\n",
      "Somalia         -->    COUNTRY\n",
      "Rwanda          -->    COUNTRY\n",
      "Singapore       -->    COUNTRY\n",
      "Sierra Leone    -->    COUNTRY\n",
      "Afghanistan     -->    COUNTRY\n",
      "Iraq            -->    COUNTRY\n",
      "Sudan           -->    COUNTRY\n",
      "Congo           -->    COUNTRY\n",
      "Haiti           -->    COUNTRY\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(text)\n",
    "#ruler = EntityRuler(nlp) \n",
    "#patterns = list(nlp.pipe(list_of_countries))\n",
    "#ruler.add_patterns(patterns)\n",
    "#nlp.add_pipe(ruler)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_=='COUNTRY':\n",
    "        print(f'{ent.text:{15}} --> {ent.label_:>{10}}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Cold War              --> ..........EVENT\n",
      "UN                        --> ............ORG\n",
      "ten years                 --> ...........DATE\n",
      "the previous four decades --> ...........DATE\n",
      "Between 1988 and 2000     --> ...........DATE\n",
      "Security Council          --> ............ORG\n",
      "UN                        --> ............ORG\n",
      "the Salvadoran Civil War  --> ..........EVENT\n",
      "Namibia                   --> ........COUNTRY\n",
      "South Africa              --> ........COUNTRY\n",
      "Cambodia                  --> ........COUNTRY\n",
      "Kuwait                    --> ........COUNTRY\n",
      "Somalia                   --> ........COUNTRY\n",
      "Haiti                     --> ........COUNTRY\n",
      "Mozambique                --> ........COUNTRY\n",
      "Somalia                   --> ........COUNTRY\n",
      "Rwanda                    --> ........COUNTRY\n",
      "Singapore                 --> ........COUNTRY\n",
      "Sierra Leone              --> ........COUNTRY\n",
      "Afghanistan               --> ........COUNTRY\n",
      "Iraq                      --> ........COUNTRY\n",
      "Sudan                     --> ........COUNTRY\n",
      "Congo                     --> ........COUNTRY\n",
      "Haiti                     --> ........COUNTRY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents: # not working properly, can not catch the dates and other GPE such as New York and Ankara\n",
    "    print(f'{ent.text:{25}} --> {ent.label_:.>{15}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## We are using the COUNTRIES matcher described above\n",
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "# matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# from spacy.tokens import Doc, Span\n",
    "# for match_id, start, end in matcher(doc):\n",
    "    \n",
    "#     # Create a span with the label of \"GPE\"\n",
    "#     span = Span(doc2, start, end, label='GPE')\n",
    "#     doc.ents = list(doc.ents) + [span]\n",
    "#     #print(doc.ents)\n",
    "# # Print the only COUNTRY entities in the document\n",
    "# print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'entity_matcher', 'ner']"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x217c8a588>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x18b1f87c8>), ('entity_matcher', <__main__.EntityMatcher object at 0x2964d9b38>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x18b1f8828>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp('This is a sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL')\n",
    "             for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting Custom Attributes\n",
    "\n",
    "from spacy.tokens import Doc, Token, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n",
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed, force=True) # If you don't put foce=True, you will get an error \n",
    "# as of 2nd run\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 26 tokens long.\n",
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def countries_component(doc):\n",
    "#     # Create an entity Span with the label 'GPE' for all matches\n",
    "#     doc.ents = [Span(doc, start, end, label='GPE')\n",
    "#                 for match_id, start, end in matcher(doc)]\n",
    "#     return doc\n",
    "\n",
    "# # Add the component to the pipeline\n",
    "# nlp.add_pipe(countries_component)\n",
    "\n",
    "# # Register capital and getter that looks up the span text in country capitals\n",
    "# Span.set_extension('capital', getter=lambda span: capitals.get(span.text), force=True)\n",
    "\n",
    "# # Process the text and print the entity text, label and capital attributes\n",
    "# doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "# print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BAD \n",
    "\n",
    "docs =[nlp(text) for text in LOTS_OF_TEXT]\n",
    "\n",
    "## GOOD\n",
    "\n",
    "docs = list(nlp.pipe(LOT_SOF_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('This is a text', {'id':1, 'page_number':15}),\n",
    "        ('And another text', {'id':2, 'page_number':16})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF WE ONLY NEED TEXT ATTRIBUTE/COMPONENT OF NLP THEN:\n",
    "## BAD \n",
    "\n",
    "doc = nlp('Hello world')\n",
    "\n",
    "## GOOD\n",
    "\n",
    "doc = nlp.make_doc(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello world, I am Serdar from USA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Serdar, USA)\n"
     ]
    }
   ],
   "source": [
    "## WE CAN TEMPORARILY DISABLE SOME COMPONENTS TO FASTER PROCESS\n",
    "\n",
    "with nlp.disable_pipes('tagger', 'parser'): # out of with block they will be restored\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the adjectives\n",
    "\n",
    "## This is inefficient way\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "# Efficient way\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (@McDonalds,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['How to preorder the iPhone X',\n",
    " 'iPhone X is coming',\n",
    " 'Should I pay $1,000 for the iPhone X?',\n",
    " 'The iPhone 8 reviews are here',\n",
    " 'Your iPhone goes up to 11 today',\n",
    " 'I need a new phone! Any tips?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'OP':'?', 'IS_DIGIT': True}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET', None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]}),\n",
    " ('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]}),\n",
    " ('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]}),\n",
    " ('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]}),\n",
    " ('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]}),\n",
    " ('I need a new phone! Any tips?', {'entities': []})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.672516345977783}\n",
      "{'ner': 9.683455526828766}\n",
      "{'ner': 12.075345039367676}\n",
      "{'ner': 1.2624063352122903}\n",
      "{'ner': 3.51289764046669}\n",
      "{'ner': 5.7956129572412465}\n",
      "{'ner': 0.4537782919669553}\n",
      "{'ner': 1.980706626929532}\n",
      "{'ner': 3.642047315284799}\n",
      "{'ner': 1.1778354898560792}\n",
      "{'ner': 1.9670406673976686}\n",
      "{'ner': 2.088610317414954}\n",
      "{'ner': 0.20141411817726151}\n",
      "{'ner': 0.21626807155368288}\n",
      "{'ner': 1.6583324711172316}\n",
      "{'ner': 0.009449234382515215}\n",
      "{'ner': 0.012873385455405728}\n",
      "{'ner': 1.0180944650129042}\n",
      "{'ner': 0.00017365360730181578}\n",
      "{'ner': 0.0018220643187084606}\n",
      "{'ner': 0.04926770868093577}\n",
      "{'ner': 0.0002627248097750723}\n",
      "{'ner': 0.0013336976211744123}\n",
      "{'ner': 0.020566556761281607}\n",
      "{'ner': 5.074081209199832e-06}\n",
      "{'ner': 0.00013164226499752865}\n",
      "{'ner': 0.0005426717944250377}\n",
      "{'ner': 2.7057295832466632e-06}\n",
      "{'ner': 0.00011574770890507291}\n",
      "{'ner': 0.00011914996419994106}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you've successfully trained your first spaCy model. The numbers printed to the IPython shell represent the loss on each iteration, the amount of work left for the optimizer. The lower the number, the better. In real life, you normally want to use a lot more data than this, ideally at least a few hundred or a few thousand examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = ['Apple is slowing down the iPhone 8 and iPhone X - how to stop it',\n",
    " \"I finally understand what the iPhone X 'notch' is for\",\n",
    " 'Everything you need to know about the Samsung Galaxy S9',\n",
    " 'Looking to compare iPad models? Here’s how the 2018 lineup stacks up',\n",
    " 'The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple',\n",
    " 'what is the cheapest ipad, especially ipad pro???',\n",
    " 'Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is slowing down the iPhone 8 and iPhone X - how to stop it\n",
      "(iPhone 8, iPhone X) \n",
      "\n",
      "I finally understand what the iPhone X 'notch' is for\n",
      "(iPhone X,) \n",
      "\n",
      "Everything you need to know about the Samsung Galaxy S9\n",
      "() \n",
      "\n",
      "Looking to compare iPad models? Here’s how the 2018 lineup stacks up\n",
      "() \n",
      "\n",
      "The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\n",
      "(iPhone 8, iPhone 8) \n",
      "\n",
      "what is the cheapest ipad, especially ipad pro???\n",
      "() \n",
      "\n",
      "Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\n",
      "() \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each text in TEST_DATA\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print(doc.ents, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(0, 9, 'PERSON'), (18, 25, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (15, 29, 'PERSON')]}),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
