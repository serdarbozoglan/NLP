{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serdarbozoglan/NLP/blob/master/Keras/Text_Classification2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w8ceA39mptiF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import reuters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CZfbIPip-mP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "word_index = reuters.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z3Smwvm1q6Tz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "if set a number to num_words it will bring the **most common **that much word. In this case (**None**) it brings all the words in the dataset"
      ]
    },
    {
      "metadata": {
        "id": "suN5HtA9rV3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1496
        },
        "outputId": "08b5881e-a791-46fa-ee67-48cb5689e494"
      },
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 27595,\n",
              " 28842,\n",
              " 8,\n",
              " 43,\n",
              " 10,\n",
              " 447,\n",
              " 5,\n",
              " 25,\n",
              " 207,\n",
              " 270,\n",
              " 5,\n",
              " 3095,\n",
              " 111,\n",
              " 16,\n",
              " 369,\n",
              " 186,\n",
              " 90,\n",
              " 67,\n",
              " 7,\n",
              " 89,\n",
              " 5,\n",
              " 19,\n",
              " 102,\n",
              " 6,\n",
              " 19,\n",
              " 124,\n",
              " 15,\n",
              " 90,\n",
              " 67,\n",
              " 84,\n",
              " 22,\n",
              " 482,\n",
              " 26,\n",
              " 7,\n",
              " 48,\n",
              " 4,\n",
              " 49,\n",
              " 8,\n",
              " 864,\n",
              " 39,\n",
              " 209,\n",
              " 154,\n",
              " 6,\n",
              " 151,\n",
              " 6,\n",
              " 83,\n",
              " 11,\n",
              " 15,\n",
              " 22,\n",
              " 155,\n",
              " 11,\n",
              " 15,\n",
              " 7,\n",
              " 48,\n",
              " 9,\n",
              " 4579,\n",
              " 1005,\n",
              " 504,\n",
              " 6,\n",
              " 258,\n",
              " 6,\n",
              " 272,\n",
              " 11,\n",
              " 15,\n",
              " 22,\n",
              " 134,\n",
              " 44,\n",
              " 11,\n",
              " 15,\n",
              " 16,\n",
              " 8,\n",
              " 197,\n",
              " 1245,\n",
              " 90,\n",
              " 67,\n",
              " 52,\n",
              " 29,\n",
              " 209,\n",
              " 30,\n",
              " 32,\n",
              " 132,\n",
              " 6,\n",
              " 109,\n",
              " 15,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "le0I9OV8rYM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a463de1-5b51-4778-ee70-68b466d77b6d"
      },
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "xM3MhRjtrbxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f20c9409-e22b-45e6-ad3c-c028bb3bc263"
      },
      "cell_type": "code",
      "source": [
        "type(y_train)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "FwrnPSXIrhOD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "3ec43e40-fd45-4d8a-fd5c-02273fda5477"
      },
      "cell_type": "code",
      "source": [
        "set(y_train)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "dFJS_rB3rr5-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have** 46 different labels **for the news"
      ]
    },
    {
      "metadata": {
        "id": "8ELCxGmErqLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a95f4be0-e5f4-4a8b-b7ad-ec969b770673"
      },
      "cell_type": "code",
      "source": [
        "print(f\"# of Training Sentences {len(X_train)}\")\n",
        "print(f\"# of Test Sentences     {len(X_test)}\")\n",
        "num_classes = max(y_train) +1 # due to class name 0, we add 1\n",
        "print(f\"# of classess             {num_classes}\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of Training Sentences 8982\n",
            "# of Test Sentences     2246\n",
            "# of classess             46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UEMklg9VsgVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "09ff8b72-fbb2-4705-f8e1-e63fff3c7fe3"
      },
      "cell_type": "code",
      "source": [
        "print(X_train[0]) # the sequence of most frequent words.\n",
        "print(y_train[0]) # label number is 3"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LCvDsgebtAOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb12d4be-f982-4f2e-d13b-7097dd35ed24"
      },
      "cell_type": "code",
      "source": [
        "word_index['money']\n",
        "# 'money' is tge 236th most frequent word in our corpus"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "job90PxitVE0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# let's check from index to word\n",
        "\n",
        "index_to_word = {}\n",
        "\n",
        "# we will flip the dictionary\n",
        "for key, value in word_index.items():\n",
        "  index_to_word[value] = key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdxR4b5FuTHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bd399f1-94a3-4202-b0fb-8b6a1590728f"
      },
      "cell_type": "code",
      "source": [
        "index_to_word[236]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'money'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "P00TMkIeuaUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d601e963-e9a1-4695-a0ab-b27a5c1d4950"
      },
      "cell_type": "code",
      "source": [
        "print(\" \".join([index_to_word[x] for x in X_train[0]]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "npVZlroeunBW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "max_words=10000 # we want only most frequent 10000 words\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "\n",
        "X_train = tokenizer.sequences_to_matrix(X_train, mode='binary') \n",
        "# binary is used for if the word in first 10000 it will get 1, otherwise 0\n",
        "\n",
        "X_test = tokenizer.sequences_to_matrix(X_test, mode='binary') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzlr_uDDw7Fk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pwtbj_m239IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "610637c7-d007-4aa3-8e1c-5e958a0f20da"
      },
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(X_train[0])\n",
        "\n",
        "#print(f\"X_test shape : {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(y_train[0]) # 46 lable is one-hot encoded, 3rd label is shown as 1\n",
        "#print(f\"y_test shape : {y_test.shape}\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (8982, 10000)\n",
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "y_train shape: (8982, 46)\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lHLq8f2z4FFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5aa18671-c80a-42dc-ea32-a72f6e6bb80f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sum(X_train[0]) \n",
        "# there are 56 words from 10000 word-vector in the first document"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "metadata": {
        "id": "T82hAD505RDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohRY_aQD5yYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1d0f8493-49b2-4562-d4fb-206250dc1215"
      },
      "cell_type": "code",
      "source": [
        "model =Sequential()\n",
        "\n",
        "model.add(Dense(512, input_shape=(max_words, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0319 17:06:10.789990 140420596017024 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NJESdJ076hRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "888f5052-d415-4ecd-8994-b6f37437f448"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.metrics_names)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'acc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SMI8wUzl7Cxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7dc1bde9-cafe-4e4d-a80f-138619e95993"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print(f\"Test loss     :{score[0]}\")\n",
        "print(f\"Test accuracy :{score[1]}\")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/2\n",
            "8083/8083 [==============================] - 25s 3ms/step - loss: 0.1805 - acc: 0.9547 - val_loss: 1.0780 - val_acc: 0.8053\n",
            "Epoch 2/2\n",
            "8083/8083 [==============================] - 25s 3ms/step - loss: 0.1714 - acc: 0.9576 - val_loss: 1.0622 - val_acc: 0.8020\n",
            "2246/2246 [==============================] - 1s 519us/step\n",
            "Test loss     :1.048913890935434\n",
            "Test accuracy :0.8000890472215515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AjmacYgM7dKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0874b472-72b3-4e5d-8b9c-49941bf931e6"
      },
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n",
        "print(max(X_train[0]))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LgDRcXE4_9lL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## This part will be a kinda repitation. We will compare tokenixation method\n",
        "## And will compare the performance\n",
        "\n",
        "#  TOKENIZATION FOR BINARY REPRESENTATION OF WORDS (EITHER IN 10000 WORDS NOT NOT)\n",
        "\n",
        "X_train = tokenizer.sequences_to_matrix(X_train, mode='binary') \n",
        "# binary is used for if the word in first 10000 it will get 1, otherwise 0\n",
        "\n",
        "X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tf6MrO78CSvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "29494435-eb9c-421c-ff14-d4d6670e6ab9"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(f\"Test accuracy {score[1]}\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/2\n",
            "8083/8083 [==============================] - 25s 3ms/step - loss: 0.9446 - acc: 0.7955 - val_loss: 0.8450 - val_acc: 0.8209\n",
            "Epoch 2/2\n",
            "8083/8083 [==============================] - 24s 3ms/step - loss: 0.4111 - acc: 0.9077 - val_loss: 0.8261 - val_acc: 0.8220\n",
            "2246/2246 [==============================] - 1s 564us/step\n",
            "Test accuracy 0.808993766696349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RJfur7E0D3qF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TOKENIZATION FOR COUNT REPRESENTATION OF WORDS "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgrYTBvPC7QY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b2870dab-1988-42b5-d7b6-60d257ba61b7"
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "X_train = tokenizer.sequences_to_matrix(X_train, mode='count') \n",
        "X_test = tokenizer.sequences_to_matrix(X_test, mode='count')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(X_train[0])\n",
        "print(max(X_train[0]))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4C0X3wQlEytf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time we convert mode from Binary to Count. This ** counts all 1s** in the vector. Means brings the number of words which are already in 10000 words."
      ]
    },
    {
      "metadata": {
        "id": "hG3x65vLEjkH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYKsirB0Er6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "0ff20c01-f41f-4bf3-e471-d4fa4e5a614f"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(f\"Test accuracy {score[1]}\")"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/2\n",
            "8083/8083 [==============================] - 25s 3ms/step - loss: 1.3358 - acc: 0.7221 - val_loss: 0.9628 - val_acc: 0.8053\n",
            "Epoch 2/2\n",
            "8083/8083 [==============================] - 24s 3ms/step - loss: 0.5842 - acc: 0.8762 - val_loss: 0.9054 - val_acc: 0.8209\n",
            "2246/2246 [==============================] - 1s 514us/step\n",
            "Test accuracy 0.8121104185218165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jkrSZ613EvNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TOKENIZATION FOR FREQUENCY REPRESENTATION OF WORDS "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjkAkFi4FUd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dcf68654-e435-4317-ab98-b67402cc9547"
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "X_train = tokenizer.sequences_to_matrix(X_train, mode='freq') \n",
        "X_test = tokenizer.sequences_to_matrix(X_test, mode='freq')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(X_train[0])\n",
        "print(max(X_train[0]))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.01149425 0.         ... 0.         0.         0.        ]\n",
            "0.06896551724137931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "keNa4PNWFY5O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is very similar to Count but it is **scaled**"
      ]
    },
    {
      "metadata": {
        "id": "BfYGdOpYFX60",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nS56gLl7JLZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "531f81a3-ea3b-4145-daa2-7fd33b7ccaef"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(f\"Test accuracy {score[1]}\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/2\n",
            "8083/8083 [==============================] - 24s 3ms/step - loss: 2.3287 - acc: 0.4398 - val_loss: 1.9128 - val_acc: 0.5006\n",
            "Epoch 2/2\n",
            "8083/8083 [==============================] - 23s 3ms/step - loss: 1.7251 - acc: 0.5510 - val_loss: 1.6440 - val_acc: 0.5951\n",
            "2246/2246 [==============================] - 1s 504us/step\n",
            "Test accuracy 0.5890471950398952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TiLcYtk4KFos",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TOKENIZATION FOR TFIDF REPRESENTATION OF WORDS "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KC2t5MQnJN9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1376180f-238c-4de0-8162-e5af2883d12e"
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "\n",
        "tokenizer.fit_on_sequences(X_train)\n",
        "\n",
        "X_train = tokenizer.sequences_to_matrix(X_train, mode='tfidf') \n",
        "X_test = tokenizer.sequences_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(X_train[0])\n",
        "print(max(X_train[0]))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.69309152 0.         ... 0.         0.         0.        ]\n",
            "6.214608098422191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vizAXk9MJgyq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTrEm9lxJ9n2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b94e9734-c28a-4007-e26e-d39653739f7c"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(f\"Test accuracy {score[1]}\")"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/2\n",
            "8083/8083 [==============================] - 30s 4ms/step - loss: 1.2552 - acc: 0.7553 - val_loss: 0.9498 - val_acc: 0.8276\n",
            "Epoch 2/2\n",
            "8083/8083 [==============================] - 30s 4ms/step - loss: 0.4506 - acc: 0.9155 - val_loss: 1.0676 - val_acc: 0.8120\n",
            "2246/2246 [==============================] - 1s 508us/step\n",
            "Test accuracy 0.8045414069456812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-qayzmQLLU-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4db6fff3-8ff1-42b8-dc64-478d63dc297b"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "metadata": {
        "id": "j3v4UHWtKAkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3e663ca3-90ee-429f-99e8-ca6bf1b15031"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Initializing the list of synnonyms and antonyms\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"good\"):\n",
        "    for s in syn.lemmas():\n",
        "        synonyms.append(s.name())\n",
        "        for a in s.antonyms():\n",
        "            antonyms.append(a.name())\n",
        "            \n",
        "            \n",
        "# Displaying the synonyms and antonyms\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'skillful', 'practiced', 'soundly', 'unspoilt', 'goodness', 'dear', 'full', 'thoroughly', 'respectable', 'just', 'skilful', 'beneficial', 'good', 'dependable', 'proficient', 'secure', 'trade_good', 'honest', 'ripe', 'expert', 'well', 'effective', 'serious', 'in_force', 'commodity', 'undecomposed', 'sound', 'salutary', 'upright', 'near', 'safe', 'honorable', 'adept', 'unspoiled', 'right', 'in_effect', 'estimable'}\n",
            "{'bad', 'evilness', 'ill', 'badness', 'evil'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mZ_ZKoC3LPrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}