{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/serdarbozoglan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/serdarbozoglan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/serdarbozoglan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/serdarbozoglan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.calibrationdatasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris =load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encoding\n",
    "# class 0 --> [1,0,0]\n",
    "# class 1 --> [0,1,0]\n",
    "# class 2 --> [0,0,1]\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.7, 2.9, 4.2, 1.3],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [7.1, 3. , 5.9, 2.1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale for ONLY TRAINING DATA\n",
    "scaler_object.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_object.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41176471, 0.40909091, 0.55357143, 0.5       ],\n",
       "       [0.97058824, 0.45454545, 0.98214286, 0.83333333],\n",
       "       [0.38235294, 0.45454545, 0.60714286, 0.58333333],\n",
       "       [0.23529412, 0.68181818, 0.05357143, 0.04166667],\n",
       "       [1.        , 0.36363636, 1.        , 0.79166667],\n",
       "       [0.44117647, 0.31818182, 0.53571429, 0.375     ],\n",
       "       [0.26470588, 0.63636364, 0.05357143, 0.04166667],\n",
       "       [0.20588235, 0.68181818, 0.03571429, 0.08333333],\n",
       "       [0.23529412, 0.81818182, 0.14285714, 0.125     ],\n",
       "       [0.20588235, 0.        , 0.42857143, 0.375     ],\n",
       "       [0.58823529, 0.31818182, 0.67857143, 0.70833333],\n",
       "       [0.14705882, 0.63636364, 0.14285714, 0.04166667],\n",
       "       [0.20588235, 0.45454545, 0.08928571, 0.04166667],\n",
       "       [0.23529412, 0.59090909, 0.10714286, 0.16666667],\n",
       "       [0.38235294, 0.31818182, 0.55357143, 0.5       ],\n",
       "       [0.23529412, 0.63636364, 0.07142857, 0.04166667],\n",
       "       [0.41176471, 0.45454545, 0.55357143, 0.45833333],\n",
       "       [1.        , 0.81818182, 1.        , 0.875     ],\n",
       "       [0.08823529, 0.54545455, 0.05357143, 0.04166667],\n",
       "       [0.55882353, 0.40909091, 0.57142857, 0.5       ],\n",
       "       [0.41176471, 0.22727273, 0.69642857, 0.79166667],\n",
       "       [0.35294118, 1.        , 0.05357143, 0.04166667],\n",
       "       [0.5       , 0.45454545, 0.66071429, 0.70833333],\n",
       "       [0.44117647, 0.31818182, 0.71428571, 0.75      ],\n",
       "       [0.5       , 0.09090909, 0.51785714, 0.375     ],\n",
       "       [0.32352941, 0.45454545, 0.60714286, 0.58333333],\n",
       "       [0.55882353, 0.63636364, 0.76785714, 0.91666667],\n",
       "       [0.35294118, 0.13636364, 0.51785714, 0.5       ],\n",
       "       [0.32352941, 0.86363636, 0.10714286, 0.125     ],\n",
       "       [0.20588235, 0.13636364, 0.39285714, 0.375     ],\n",
       "       [0.61764706, 0.31818182, 0.75      , 0.75      ],\n",
       "       [0.20588235, 0.59090909, 0.05357143, 0.04166667],\n",
       "       [0.20588235, 0.54545455, 0.01785714, 0.04166667],\n",
       "       [0.35294118, 0.18181818, 0.48214286, 0.41666667],\n",
       "       [0.70588235, 0.45454545, 0.69642857, 0.66666667],\n",
       "       [0.17647059, 0.5       , 0.07142857, 0.04166667],\n",
       "       [0.44117647, 0.36363636, 0.71428571, 0.95833333],\n",
       "       [0.20588235, 0.63636364, 0.07142857, 0.04166667],\n",
       "       [0.20588235, 0.68181818, 0.08928571, 0.20833333],\n",
       "       [0.47058824, 0.54545455, 0.66071429, 0.70833333],\n",
       "       [0.23529412, 0.22727273, 0.33928571, 0.41666667],\n",
       "       [0.76470588, 0.54545455, 0.82142857, 0.91666667],\n",
       "       [0.5       , 0.31818182, 0.71428571, 0.625     ],\n",
       "       [0.52941176, 0.27272727, 0.80357143, 0.54166667],\n",
       "       [1.        , 0.45454545, 0.89285714, 0.91666667],\n",
       "       [0.35294118, 0.22727273, 0.51785714, 0.5       ],\n",
       "       [0.02941176, 0.40909091, 0.05357143, 0.04166667],\n",
       "       [0.        , 0.45454545, 0.        , 0.        ],\n",
       "       [0.5       , 0.09090909, 0.69642857, 0.58333333],\n",
       "       [0.85294118, 0.54545455, 0.875     , 0.70833333],\n",
       "       [0.08823529, 0.5       , 0.07142857, 0.04166667],\n",
       "       [0.23529412, 0.68181818, 0.05357143, 0.08333333],\n",
       "       [0.02941176, 0.45454545, 0.03571429, 0.04166667],\n",
       "       [0.58823529, 0.22727273, 0.67857143, 0.58333333],\n",
       "       [0.58823529, 0.63636364, 0.80357143, 0.95833333],\n",
       "       [0.08823529, 0.63636364, 0.05357143, 0.08333333],\n",
       "       [0.73529412, 0.45454545, 0.78571429, 0.83333333],\n",
       "       [0.58823529, 0.59090909, 0.875     , 1.        ],\n",
       "       [0.11764706, 0.54545455, 0.03571429, 0.04166667],\n",
       "       [0.52941176, 0.40909091, 0.64285714, 0.54166667],\n",
       "       [0.64705882, 0.36363636, 0.625     , 0.58333333],\n",
       "       [0.55882353, 0.36363636, 0.66071429, 0.70833333],\n",
       "       [0.79411765, 0.54545455, 0.64285714, 0.54166667],\n",
       "       [0.61764706, 0.54545455, 0.75      , 0.91666667],\n",
       "       [0.23529412, 0.81818182, 0.08928571, 0.04166667],\n",
       "       [0.76470588, 0.5       , 0.76785714, 0.83333333],\n",
       "       [0.47058824, 0.45454545, 0.55357143, 0.58333333],\n",
       "       [0.64705882, 0.45454545, 0.73214286, 0.79166667],\n",
       "       [0.41176471, 0.27272727, 0.42857143, 0.375     ],\n",
       "       [0.26470588, 0.31818182, 0.5       , 0.54166667],\n",
       "       [0.52941176, 0.45454545, 0.625     , 0.54166667],\n",
       "       [0.05882353, 0.13636364, 0.03571429, 0.08333333],\n",
       "       [0.67647059, 0.40909091, 0.625     , 0.5       ],\n",
       "       [0.35294118, 0.27272727, 0.58928571, 0.45833333],\n",
       "       [0.29411765, 0.77272727, 0.07142857, 0.04166667],\n",
       "       [0.38235294, 0.45454545, 0.53571429, 0.5       ],\n",
       "       [0.88235294, 0.40909091, 0.92857143, 0.70833333],\n",
       "       [0.70588235, 0.59090909, 0.82142857, 0.83333333],\n",
       "       [0.23529412, 0.77272727, 0.07142857, 0.125     ],\n",
       "       [0.17647059, 0.18181818, 0.39285714, 0.375     ],\n",
       "       [0.70588235, 0.59090909, 0.82142857, 1.        ],\n",
       "       [0.85294118, 0.45454545, 0.83928571, 0.625     ],\n",
       "       [0.17647059, 0.72727273, 0.05357143, 0.        ],\n",
       "       [0.70588235, 0.5       , 0.80357143, 0.95833333],\n",
       "       [0.17647059, 0.45454545, 0.05357143, 0.04166667],\n",
       "       [0.76470588, 0.5       , 0.67857143, 0.58333333],\n",
       "       [0.91176471, 0.36363636, 0.89285714, 0.75      ],\n",
       "       [0.58823529, 0.40909091, 0.80357143, 0.70833333],\n",
       "       [0.41176471, 0.36363636, 0.53571429, 0.5       ],\n",
       "       [0.64705882, 0.45454545, 0.78571429, 0.70833333],\n",
       "       [0.58823529, 0.13636364, 0.58928571, 0.5       ],\n",
       "       [0.61764706, 0.40909091, 0.57142857, 0.5       ],\n",
       "       [0.38235294, 0.36363636, 0.67857143, 0.79166667],\n",
       "       [0.47058824, 0.45454545, 0.71428571, 0.70833333],\n",
       "       [0.32352941, 0.63636364, 0.10714286, 0.04166667],\n",
       "       [0.52941176, 0.36363636, 0.51785714, 0.5       ],\n",
       "       [0.17647059, 0.22727273, 0.60714286, 0.66666667],\n",
       "       [0.44117647, 0.90909091, 0.01785714, 0.04166667],\n",
       "       [0.44117647, 0.27272727, 0.51785714, 0.45833333],\n",
       "       [0.82352941, 0.45454545, 0.85714286, 0.83333333]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu')) # layers I put here is 2xfeatures, no right answer\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) # output layer will have 3 classes\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 0s - loss: 0.5084 - acc: 0.6900\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.5064 - acc: 0.6900\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.5043 - acc: 0.6900\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.5023 - acc: 0.6900\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.4999 - acc: 0.6900\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.4979 - acc: 0.6900\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.4958 - acc: 0.7100\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.4937 - acc: 0.7100\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.4918 - acc: 0.7100\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.4900 - acc: 0.7100\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.4881 - acc: 0.7000\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.4869 - acc: 0.6900\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.4849 - acc: 0.6900\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.4829 - acc: 0.7100\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.4813 - acc: 0.7200\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.4791 - acc: 0.7200\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.4772 - acc: 0.7300\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.4761 - acc: 0.7500\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.4734 - acc: 0.7900\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.4715 - acc: 0.8200\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.4697 - acc: 0.8200\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.4681 - acc: 0.8400\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.4665 - acc: 0.8400\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.4645 - acc: 0.8400\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.4633 - acc: 0.8400\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.4612 - acc: 0.8400\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.4596 - acc: 0.8400\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.4576 - acc: 0.8400\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.4558 - acc: 0.8400\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.4544 - acc: 0.8500\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.4523 - acc: 0.8600\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.4507 - acc: 0.8800\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.4502 - acc: 0.8900\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.4479 - acc: 0.9000\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.4462 - acc: 0.9000\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.4438 - acc: 0.8900\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.4420 - acc: 0.8800\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.4397 - acc: 0.8700\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.4378 - acc: 0.8600\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.4359 - acc: 0.8600\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.4339 - acc: 0.8600\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.4320 - acc: 0.8600\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.4301 - acc: 0.8600\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.4281 - acc: 0.8600\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.4262 - acc: 0.8600\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.4238 - acc: 0.8600\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.4223 - acc: 0.8600\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.4194 - acc: 0.8500\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.4162 - acc: 0.8500\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.4121 - acc: 0.8700\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.4083 - acc: 0.8800\n",
      "Epoch 52/500\n",
      " - 0s - loss: 0.4053 - acc: 0.8800\n",
      "Epoch 53/500\n",
      " - 0s - loss: 0.4025 - acc: 0.8900\n",
      "Epoch 54/500\n",
      " - 0s - loss: 0.3999 - acc: 0.8900\n",
      "Epoch 55/500\n",
      " - 0s - loss: 0.3976 - acc: 0.9000\n",
      "Epoch 56/500\n",
      " - 0s - loss: 0.3954 - acc: 0.9100\n",
      "Epoch 57/500\n",
      " - 0s - loss: 0.3925 - acc: 0.9100\n",
      "Epoch 58/500\n",
      " - 0s - loss: 0.3904 - acc: 0.9100\n",
      "Epoch 59/500\n",
      " - 0s - loss: 0.3879 - acc: 0.9100\n",
      "Epoch 60/500\n",
      " - 0s - loss: 0.3850 - acc: 0.9100\n",
      "Epoch 61/500\n",
      " - 0s - loss: 0.3835 - acc: 0.9100\n",
      "Epoch 62/500\n",
      " - 0s - loss: 0.3808 - acc: 0.9000\n",
      "Epoch 63/500\n",
      " - 0s - loss: 0.3787 - acc: 0.9000\n",
      "Epoch 64/500\n",
      " - 0s - loss: 0.3757 - acc: 0.9100\n",
      "Epoch 65/500\n",
      " - 0s - loss: 0.3730 - acc: 0.9100\n",
      "Epoch 66/500\n",
      " - 0s - loss: 0.3711 - acc: 0.9200\n",
      "Epoch 67/500\n",
      " - 0s - loss: 0.3687 - acc: 0.9100\n",
      "Epoch 68/500\n",
      " - 0s - loss: 0.3665 - acc: 0.9100\n",
      "Epoch 69/500\n",
      " - 0s - loss: 0.3639 - acc: 0.9100\n",
      "Epoch 70/500\n",
      " - 0s - loss: 0.3620 - acc: 0.9200\n",
      "Epoch 71/500\n",
      " - 0s - loss: 0.3595 - acc: 0.9200\n",
      "Epoch 72/500\n",
      " - 0s - loss: 0.3574 - acc: 0.9100\n",
      "Epoch 73/500\n",
      " - 0s - loss: 0.3553 - acc: 0.9100\n",
      "Epoch 74/500\n",
      " - 0s - loss: 0.3533 - acc: 0.9000\n",
      "Epoch 75/500\n",
      " - 0s - loss: 0.3509 - acc: 0.9000\n",
      "Epoch 76/500\n",
      " - 0s - loss: 0.3490 - acc: 0.9200\n",
      "Epoch 77/500\n",
      " - 0s - loss: 0.3465 - acc: 0.9100\n",
      "Epoch 78/500\n",
      " - 0s - loss: 0.3453 - acc: 0.9100\n",
      "Epoch 79/500\n",
      " - 0s - loss: 0.3435 - acc: 0.9100\n",
      "Epoch 80/500\n",
      " - 0s - loss: 0.3418 - acc: 0.9100\n",
      "Epoch 81/500\n",
      " - 0s - loss: 0.3395 - acc: 0.9100\n",
      "Epoch 82/500\n",
      " - 0s - loss: 0.3362 - acc: 0.9200\n",
      "Epoch 83/500\n",
      " - 0s - loss: 0.3344 - acc: 0.9000\n",
      "Epoch 84/500\n",
      " - 0s - loss: 0.3327 - acc: 0.9000\n",
      "Epoch 85/500\n",
      " - 0s - loss: 0.3311 - acc: 0.9100\n",
      "Epoch 86/500\n",
      " - 0s - loss: 0.3292 - acc: 0.9100\n",
      "Epoch 87/500\n",
      " - 0s - loss: 0.3270 - acc: 0.9100\n",
      "Epoch 88/500\n",
      " - 0s - loss: 0.3239 - acc: 0.9100\n",
      "Epoch 89/500\n",
      " - 0s - loss: 0.3221 - acc: 0.9100\n",
      "Epoch 90/500\n",
      " - 0s - loss: 0.3202 - acc: 0.9100\n",
      "Epoch 91/500\n",
      " - 0s - loss: 0.3179 - acc: 0.9100\n",
      "Epoch 92/500\n",
      " - 0s - loss: 0.3161 - acc: 0.9100\n",
      "Epoch 93/500\n",
      " - 0s - loss: 0.3134 - acc: 0.9100\n",
      "Epoch 94/500\n",
      " - 0s - loss: 0.3119 - acc: 0.9100\n",
      "Epoch 95/500\n",
      " - 0s - loss: 0.3095 - acc: 0.9100\n",
      "Epoch 96/500\n",
      " - 0s - loss: 0.3072 - acc: 0.9100\n",
      "Epoch 97/500\n",
      " - 0s - loss: 0.3055 - acc: 0.9200\n",
      "Epoch 98/500\n",
      " - 0s - loss: 0.3035 - acc: 0.9300\n",
      "Epoch 99/500\n",
      " - 0s - loss: 0.3017 - acc: 0.9300\n",
      "Epoch 100/500\n",
      " - 0s - loss: 0.3010 - acc: 0.9200\n",
      "Epoch 101/500\n",
      " - 0s - loss: 0.2998 - acc: 0.9200\n",
      "Epoch 102/500\n",
      " - 0s - loss: 0.2973 - acc: 0.9200\n",
      "Epoch 103/500\n",
      " - 0s - loss: 0.2955 - acc: 0.9200\n",
      "Epoch 104/500\n",
      " - 0s - loss: 0.2935 - acc: 0.9200\n",
      "Epoch 105/500\n",
      " - 0s - loss: 0.2908 - acc: 0.9200\n",
      "Epoch 106/500\n",
      " - 0s - loss: 0.2890 - acc: 0.9300\n",
      "Epoch 107/500\n",
      " - 0s - loss: 0.2866 - acc: 0.9300\n",
      "Epoch 108/500\n",
      " - 0s - loss: 0.2853 - acc: 0.9300\n",
      "Epoch 109/500\n",
      " - 0s - loss: 0.2836 - acc: 0.9300\n",
      "Epoch 110/500\n",
      " - 0s - loss: 0.2817 - acc: 0.9300\n",
      "Epoch 111/500\n",
      " - 0s - loss: 0.2795 - acc: 0.9300\n",
      "Epoch 112/500\n",
      " - 0s - loss: 0.2771 - acc: 0.9100\n",
      "Epoch 113/500\n",
      " - 0s - loss: 0.2749 - acc: 0.9100\n",
      "Epoch 114/500\n",
      " - 0s - loss: 0.2744 - acc: 0.9100\n",
      "Epoch 115/500\n",
      " - 0s - loss: 0.2724 - acc: 0.9200\n",
      "Epoch 116/500\n",
      " - 0s - loss: 0.2705 - acc: 0.9200\n",
      "Epoch 117/500\n",
      " - 0s - loss: 0.2685 - acc: 0.9100\n",
      "Epoch 118/500\n",
      " - 0s - loss: 0.2689 - acc: 0.9100\n",
      "Epoch 119/500\n",
      " - 0s - loss: 0.2659 - acc: 0.9300\n",
      "Epoch 120/500\n",
      " - 0s - loss: 0.2644 - acc: 0.9300\n",
      "Epoch 121/500\n",
      " - 0s - loss: 0.2631 - acc: 0.9300\n",
      "Epoch 122/500\n",
      " - 0s - loss: 0.2619 - acc: 0.9300\n",
      "Epoch 123/500\n",
      " - 0s - loss: 0.2596 - acc: 0.9300\n",
      "Epoch 124/500\n",
      " - 0s - loss: 0.2567 - acc: 0.9200\n",
      "Epoch 125/500\n",
      " - 0s - loss: 0.2553 - acc: 0.9100\n",
      "Epoch 126/500\n",
      " - 0s - loss: 0.2538 - acc: 0.9300\n",
      "Epoch 127/500\n",
      " - 0s - loss: 0.2556 - acc: 0.9300\n",
      "Epoch 128/500\n",
      " - 0s - loss: 0.2549 - acc: 0.9300\n",
      "Epoch 129/500\n",
      " - 0s - loss: 0.2526 - acc: 0.9300\n",
      "Epoch 130/500\n",
      " - 0s - loss: 0.2496 - acc: 0.9200\n",
      "Epoch 131/500\n",
      " - 0s - loss: 0.2469 - acc: 0.9200\n",
      "Epoch 132/500\n",
      " - 0s - loss: 0.2453 - acc: 0.9200\n",
      "Epoch 133/500\n",
      " - 0s - loss: 0.2434 - acc: 0.9200\n",
      "Epoch 134/500\n",
      " - 0s - loss: 0.2421 - acc: 0.9100\n",
      "Epoch 135/500\n",
      " - 0s - loss: 0.2417 - acc: 0.9300\n",
      "Epoch 136/500\n",
      " - 0s - loss: 0.2416 - acc: 0.9300\n",
      "Epoch 137/500\n",
      " - 0s - loss: 0.2408 - acc: 0.9300\n",
      "Epoch 138/500\n",
      " - 0s - loss: 0.2387 - acc: 0.9300\n",
      "Epoch 139/500\n",
      " - 0s - loss: 0.2366 - acc: 0.9300\n",
      "Epoch 140/500\n",
      " - 0s - loss: 0.2344 - acc: 0.9100\n",
      "Epoch 141/500\n",
      " - 0s - loss: 0.2331 - acc: 0.9200\n",
      "Epoch 142/500\n",
      " - 0s - loss: 0.2321 - acc: 0.9300\n",
      "Epoch 143/500\n",
      " - 0s - loss: 0.2308 - acc: 0.9300\n",
      "Epoch 144/500\n",
      " - 0s - loss: 0.2301 - acc: 0.9300\n",
      "Epoch 145/500\n",
      " - 0s - loss: 0.2287 - acc: 0.9300\n",
      "Epoch 146/500\n",
      " - 0s - loss: 0.2270 - acc: 0.9300\n",
      "Epoch 147/500\n",
      " - 0s - loss: 0.2253 - acc: 0.9300\n",
      "Epoch 148/500\n",
      " - 0s - loss: 0.2237 - acc: 0.9300\n",
      "Epoch 149/500\n",
      " - 0s - loss: 0.2221 - acc: 0.9300\n",
      "Epoch 150/500\n",
      " - 0s - loss: 0.2210 - acc: 0.9200\n",
      "Epoch 151/500\n",
      " - 0s - loss: 0.2194 - acc: 0.9200\n",
      "Epoch 152/500\n",
      " - 0s - loss: 0.2181 - acc: 0.9200\n",
      "Epoch 153/500\n",
      " - 0s - loss: 0.2174 - acc: 0.9300\n",
      "Epoch 154/500\n",
      " - 0s - loss: 0.2161 - acc: 0.9300\n",
      "Epoch 155/500\n",
      " - 0s - loss: 0.2145 - acc: 0.9300\n",
      "Epoch 156/500\n",
      " - 0s - loss: 0.2135 - acc: 0.9200\n",
      "Epoch 157/500\n",
      " - 0s - loss: 0.2120 - acc: 0.9200\n",
      "Epoch 158/500\n",
      " - 0s - loss: 0.2110 - acc: 0.9200\n",
      "Epoch 159/500\n",
      " - 0s - loss: 0.2099 - acc: 0.9200\n",
      "Epoch 160/500\n",
      " - 0s - loss: 0.2089 - acc: 0.9200\n",
      "Epoch 161/500\n",
      " - 0s - loss: 0.2076 - acc: 0.9200\n",
      "Epoch 162/500\n",
      " - 0s - loss: 0.2062 - acc: 0.9300\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.2058 - acc: 0.9300\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.2057 - acc: 0.9300\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.2061 - acc: 0.9300\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.2046 - acc: 0.9300\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.2009 - acc: 0.9300\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.1997 - acc: 0.9300\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.1988 - acc: 0.9300\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.1976 - acc: 0.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      " - 0s - loss: 0.1971 - acc: 0.9200\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.1964 - acc: 0.9200\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.1954 - acc: 0.9200\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.1944 - acc: 0.9200\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.1933 - acc: 0.9300\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.1922 - acc: 0.9200\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.1915 - acc: 0.9300\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.1910 - acc: 0.9400\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.1905 - acc: 0.9400\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.1895 - acc: 0.9400\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.1880 - acc: 0.9400\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.1869 - acc: 0.9300\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.1855 - acc: 0.9300\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.1848 - acc: 0.9300\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.1836 - acc: 0.9300\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.1843 - acc: 0.9400\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.1856 - acc: 0.9300\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.1856 - acc: 0.9300\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.1815 - acc: 0.9300\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.1805 - acc: 0.9300\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.1791 - acc: 0.9300\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.1783 - acc: 0.9300\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.1774 - acc: 0.9300\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.1767 - acc: 0.9300\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.1761 - acc: 0.9300\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.1754 - acc: 0.9400\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.1754 - acc: 0.9500\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.1751 - acc: 0.9400\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.1746 - acc: 0.9400\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.1729 - acc: 0.9400\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.1719 - acc: 0.9400\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.1708 - acc: 0.9300\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.1696 - acc: 0.9300\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.1697 - acc: 0.9300\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.1689 - acc: 0.9300\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.1677 - acc: 0.9300\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.1678 - acc: 0.9500\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.1677 - acc: 0.9500\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.1683 - acc: 0.9400\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.1675 - acc: 0.9400\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.1667 - acc: 0.9400\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.1648 - acc: 0.9400\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.1627 - acc: 0.9300\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.1638 - acc: 0.9200\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.1642 - acc: 0.9400\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.1637 - acc: 0.9400\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.1625 - acc: 0.9300\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.1612 - acc: 0.9300\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.1602 - acc: 0.9300\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.1594 - acc: 0.9300\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.1588 - acc: 0.9300\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.1580 - acc: 0.9300\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.1575 - acc: 0.9300\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.1568 - acc: 0.9300\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.1564 - acc: 0.9300\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.1558 - acc: 0.9300\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.1553 - acc: 0.9300\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.1545 - acc: 0.9400\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.1541 - acc: 0.9300\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.1533 - acc: 0.9300\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.1530 - acc: 0.9300\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.1526 - acc: 0.9300\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.1540 - acc: 0.9300\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.1519 - acc: 0.9300\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.1515 - acc: 0.9300\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.1503 - acc: 0.9400\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.1499 - acc: 0.9400\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.1528 - acc: 0.9500\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.1520 - acc: 0.9400\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.1500 - acc: 0.9500\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.1479 - acc: 0.9500\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.1461 - acc: 0.9400\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.1465 - acc: 0.9300\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.1480 - acc: 0.9300\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.1479 - acc: 0.9400\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.1469 - acc: 0.9300\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.1452 - acc: 0.9400\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.1440 - acc: 0.9300\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.1434 - acc: 0.9300\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.1426 - acc: 0.9300\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.1419 - acc: 0.9300\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.1415 - acc: 0.9300\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.1412 - acc: 0.9400\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.1425 - acc: 0.9500\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.1423 - acc: 0.9500\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.1416 - acc: 0.9500\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.1400 - acc: 0.9500\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.1386 - acc: 0.9400\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.1382 - acc: 0.9300\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.1380 - acc: 0.9400\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.1386 - acc: 0.9300\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.1375 - acc: 0.9400\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.1362 - acc: 0.9300\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.1353 - acc: 0.9400\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.1367 - acc: 0.9500\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.1373 - acc: 0.9500\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.1366 - acc: 0.9500\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.1356 - acc: 0.9500\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.1338 - acc: 0.9400\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.1334 - acc: 0.9300\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.1331 - acc: 0.9400\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.1325 - acc: 0.9400\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.1347 - acc: 0.9500\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.1327 - acc: 0.9500\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.1313 - acc: 0.9400\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.1311 - acc: 0.9400\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.1317 - acc: 0.9300\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.1316 - acc: 0.9400\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.1302 - acc: 0.9400\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.1295 - acc: 0.9500\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.1290 - acc: 0.9500\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.1291 - acc: 0.9400\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.1288 - acc: 0.9500\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.1296 - acc: 0.9500\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.1303 - acc: 0.9500\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.1308 - acc: 0.9500\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.1299 - acc: 0.9500\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.1274 - acc: 0.9500\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.1260 - acc: 0.9400\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.1264 - acc: 0.9500\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.1259 - acc: 0.9400\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.1264 - acc: 0.9300\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.1259 - acc: 0.9400\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.1244 - acc: 0.9500\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.1247 - acc: 0.9400\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.1238 - acc: 0.9400\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.1234 - acc: 0.9500\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.1232 - acc: 0.9500\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.1227 - acc: 0.9500\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.1224 - acc: 0.9500\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.1224 - acc: 0.9500\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.1214 - acc: 0.9400\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.1214 - acc: 0.9500\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.1213 - acc: 0.9400\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.1210 - acc: 0.9500\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.1207 - acc: 0.9500\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.1202 - acc: 0.9500\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.1204 - acc: 0.9500\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.1201 - acc: 0.9500\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.1196 - acc: 0.9600\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.1192 - acc: 0.9500\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.1187 - acc: 0.9500\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.1185 - acc: 0.9500\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.1184 - acc: 0.9600\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.1184 - acc: 0.9500\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.1173 - acc: 0.9500\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.1169 - acc: 0.9500\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.1172 - acc: 0.9500\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.1169 - acc: 0.9400\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.1166 - acc: 0.9400\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.1163 - acc: 0.9400\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.1165 - acc: 0.9500\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.1153 - acc: 0.9500\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.1152 - acc: 0.9500\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.1150 - acc: 0.9500\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.1146 - acc: 0.9500\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.1141 - acc: 0.9500\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.1135 - acc: 0.9500\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.1145 - acc: 0.9500\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.1155 - acc: 0.9600\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.1122 - acc: 0.9600\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.1140 - acc: 0.9300\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.1163 - acc: 0.9400\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.1196 - acc: 0.9500\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.1195 - acc: 0.9500\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.1138 - acc: 0.9300\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.1109 - acc: 0.9500\n",
      "Epoch 338/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1114 - acc: 0.9600\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.1132 - acc: 0.9600\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.1153 - acc: 0.9600\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.1154 - acc: 0.9600\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.1135 - acc: 0.9600\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.1113 - acc: 0.9700\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.1106 - acc: 0.9600\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.1094 - acc: 0.9600\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.1088 - acc: 0.9500\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.1101 - acc: 0.9400\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.1108 - acc: 0.9300\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.1133 - acc: 0.9400\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.1125 - acc: 0.9500\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.1099 - acc: 0.9300\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.1071 - acc: 0.9500\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.1074 - acc: 0.9500\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.1075 - acc: 0.9600\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.1078 - acc: 0.9700\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.1091 - acc: 0.9700\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.1079 - acc: 0.9600\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.1072 - acc: 0.9500\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.1077 - acc: 0.9400\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.1074 - acc: 0.9300\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.1070 - acc: 0.9300\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.1058 - acc: 0.9500\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.1056 - acc: 0.9500\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.1050 - acc: 0.9700\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.1059 - acc: 0.9700\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.1063 - acc: 0.9700\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.1052 - acc: 0.9700\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.1043 - acc: 0.9700\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.1037 - acc: 0.9500\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.1042 - acc: 0.9500\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.1053 - acc: 0.9300\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.1050 - acc: 0.9300\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.1037 - acc: 0.9500\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.1035 - acc: 0.9500\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.1027 - acc: 0.9500\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.1027 - acc: 0.9500\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.1025 - acc: 0.9500\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.1024 - acc: 0.9500\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.1026 - acc: 0.9500\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.1035 - acc: 0.9300\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.1062 - acc: 0.9400\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.1063 - acc: 0.9500\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.1036 - acc: 0.9500\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.1019 - acc: 0.9300\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.1007 - acc: 0.9500\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.1005 - acc: 0.9600\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.1018 - acc: 0.9700\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.1015 - acc: 0.9700\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.1004 - acc: 0.9700\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.0998 - acc: 0.9700\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.0996 - acc: 0.9600\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.1000 - acc: 0.9500\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.1009 - acc: 0.9400\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.1019 - acc: 0.9300\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.1006 - acc: 0.9300\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.0992 - acc: 0.9400\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.0987 - acc: 0.9600\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.0987 - acc: 0.9700\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.0987 - acc: 0.9700\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.0981 - acc: 0.9700\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.0978 - acc: 0.9700\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.0976 - acc: 0.9700\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.0973 - acc: 0.9700\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.0971 - acc: 0.9600\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.0969 - acc: 0.9600\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.0966 - acc: 0.9600\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.0968 - acc: 0.9600\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.0969 - acc: 0.9500\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.0966 - acc: 0.9600\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.0964 - acc: 0.9600\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.0962 - acc: 0.9600\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.0958 - acc: 0.9700\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.0957 - acc: 0.9700\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.0958 - acc: 0.9700\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.0959 - acc: 0.9700\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.0950 - acc: 0.9600\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.0954 - acc: 0.9600\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.0957 - acc: 0.9400\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.0962 - acc: 0.9400\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.0962 - acc: 0.9400\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.0954 - acc: 0.9500\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.0954 - acc: 0.9600\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.0945 - acc: 0.9700\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.0940 - acc: 0.9700\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.0944 - acc: 0.9700\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.0944 - acc: 0.9700\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.0929 - acc: 0.9700\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.0930 - acc: 0.9600\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.0934 - acc: 0.9500\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.0954 - acc: 0.9400\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.0951 - acc: 0.9500\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.0936 - acc: 0.9500\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.0926 - acc: 0.9700\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.0928 - acc: 0.9700\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.0940 - acc: 0.9700\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.0943 - acc: 0.9700\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.0939 - acc: 0.9700\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.0924 - acc: 0.9700\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.0916 - acc: 0.9700\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.0915 - acc: 0.9700\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.0914 - acc: 0.9600\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.0912 - acc: 0.9600\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.0907 - acc: 0.9700\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.0912 - acc: 0.9700\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.0913 - acc: 0.9700\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.0910 - acc: 0.9700\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.0909 - acc: 0.9700\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.0907 - acc: 0.9700\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.0903 - acc: 0.9600\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.0904 - acc: 0.9600\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.0903 - acc: 0.9600\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.0901 - acc: 0.9600\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.0899 - acc: 0.9600\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.0895 - acc: 0.9700\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.0900 - acc: 0.9700\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.0893 - acc: 0.9700\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.0893 - acc: 0.9700\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.0887 - acc: 0.9700\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.0894 - acc: 0.9700\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.0896 - acc: 0.9700\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.0894 - acc: 0.9700\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.0890 - acc: 0.9700\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.0887 - acc: 0.9700\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.0886 - acc: 0.9700\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.0886 - acc: 0.9700\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.0882 - acc: 0.9700\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.0884 - acc: 0.9700\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.0882 - acc: 0.9700\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.0870 - acc: 0.9700\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.0887 - acc: 0.9600\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.0903 - acc: 0.9500\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.0907 - acc: 0.9500\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.0905 - acc: 0.9500\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.0871 - acc: 0.9700\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.0872 - acc: 0.9700\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.0871 - acc: 0.9700\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.0870 - acc: 0.9700\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.0869 - acc: 0.9700\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.0872 - acc: 0.9700\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.0869 - acc: 0.9700\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.0866 - acc: 0.9700\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.0862 - acc: 0.9700\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.0865 - acc: 0.9700\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.0863 - acc: 0.9700\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.0867 - acc: 0.9700\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.0871 - acc: 0.9700\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.0870 - acc: 0.9700\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.0869 - acc: 0.9700\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.0880 - acc: 0.9700\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.0879 - acc: 0.9700\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.0871 - acc: 0.9700\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.0841 - acc: 0.9700\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.0855 - acc: 0.9700\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.0879 - acc: 0.9500\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.0885 - acc: 0.9400\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.0861 - acc: 0.9600\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.0836 - acc: 0.9700\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.0842 - acc: 0.9700\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.0845 - acc: 0.9700\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.0842 - acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b0b3b00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X_train, y_train, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52941176,  0.36363636,  0.64285714,  0.45833333],\n",
       "       [ 0.41176471,  0.81818182,  0.10714286,  0.08333333],\n",
       "       [ 1.        ,  0.27272727,  1.03571429,  0.91666667],\n",
       "       [ 0.5       ,  0.40909091,  0.60714286,  0.58333333],\n",
       "       [ 0.73529412,  0.36363636,  0.66071429,  0.54166667],\n",
       "       [ 0.32352941,  0.63636364,  0.07142857,  0.125     ],\n",
       "       [ 0.38235294,  0.40909091,  0.44642857,  0.5       ],\n",
       "       [ 0.76470588,  0.5       ,  0.71428571,  0.91666667],\n",
       "       [ 0.55882353,  0.09090909,  0.60714286,  0.58333333],\n",
       "       [ 0.44117647,  0.31818182,  0.5       ,  0.45833333],\n",
       "       [ 0.64705882,  0.54545455,  0.71428571,  0.79166667],\n",
       "       [ 0.14705882,  0.45454545,  0.05357143,  0.        ],\n",
       "       [ 0.35294118,  0.68181818,  0.03571429,  0.04166667],\n",
       "       [ 0.17647059,  0.5       ,  0.07142857,  0.        ],\n",
       "       [ 0.23529412,  0.81818182,  0.07142857,  0.08333333],\n",
       "       [ 0.58823529,  0.59090909,  0.64285714,  0.625     ],\n",
       "       [ 0.64705882,  0.45454545,  0.83928571,  0.875     ],\n",
       "       [ 0.38235294,  0.22727273,  0.5       ,  0.41666667],\n",
       "       [ 0.41176471,  0.36363636,  0.60714286,  0.5       ],\n",
       "       [ 0.61764706,  0.36363636,  0.80357143,  0.875     ],\n",
       "       [ 0.11764706,  0.54545455,  0.08928571,  0.04166667],\n",
       "       [ 0.52941176,  0.45454545,  0.67857143,  0.70833333],\n",
       "       [ 0.20588235,  0.63636364,  0.08928571,  0.125     ],\n",
       "       [ 0.61764706,  0.36363636,  0.80357143,  0.83333333],\n",
       "       [ 1.05882353,  0.81818182,  0.94642857,  0.79166667],\n",
       "       [ 0.70588235,  0.45454545,  0.73214286,  0.91666667],\n",
       "       [ 0.70588235,  0.22727273,  0.83928571,  0.70833333],\n",
       "       [ 0.73529412,  0.54545455,  0.85714286,  0.91666667],\n",
       "       [ 0.14705882,  0.45454545,  0.05357143,  0.08333333],\n",
       "       [ 0.14705882,  0.5       ,  0.08928571,  0.04166667],\n",
       "       [ 0.08823529,  0.72727273, -0.01785714,  0.04166667],\n",
       "       [ 0.41176471,  1.09090909,  0.07142857,  0.125     ],\n",
       "       [ 0.70588235,  0.5       ,  0.58928571,  0.54166667],\n",
       "       [ 0.14705882,  0.63636364,  0.08928571,  0.04166667],\n",
       "       [ 0.02941176,  0.54545455,  0.03571429,  0.04166667],\n",
       "       [ 0.58823529,  0.22727273,  0.69642857,  0.75      ],\n",
       "       [ 0.61764706,  0.54545455,  0.60714286,  0.58333333],\n",
       "       [ 0.26470588,  0.68181818,  0.07142857,  0.04166667],\n",
       "       [ 0.20588235,  0.72727273,  0.05357143,  0.04166667],\n",
       "       [ 0.26470588,  0.95454545,  0.07142857,  0.        ],\n",
       "       [ 0.44117647,  0.31818182,  0.71428571,  0.75      ],\n",
       "       [ 0.5       ,  0.63636364,  0.60714286,  0.625     ],\n",
       "       [ 0.70588235,  0.5       ,  0.64285714,  0.58333333],\n",
       "       [ 0.32352941,  0.86363636,  0.03571429,  0.125     ],\n",
       "       [ 0.32352941,  0.77272727,  0.07142857,  0.04166667],\n",
       "       [ 0.35294118,  0.18181818,  0.46428571,  0.375     ],\n",
       "       [ 0.58823529,  0.36363636,  0.71428571,  0.58333333],\n",
       "       [ 0.61764706,  0.5       ,  0.78571429,  0.70833333],\n",
       "       [ 0.67647059,  0.45454545,  0.58928571,  0.54166667],\n",
       "       [ 0.85294118,  0.72727273,  0.89285714,  1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a unscaled new data you MUST SCALE IT FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.13164021e-04, 9.81757462e-01, 1.80293806e-02],\n",
       "       [9.97433126e-01, 2.56691920e-03, 1.13749132e-11],\n",
       "       [5.46677829e-13, 7.13184418e-04, 9.99286830e-01],\n",
       "       [7.33969864e-05, 9.08973753e-01, 9.09529105e-02],\n",
       "       [9.80325513e-06, 7.89654672e-01, 2.10335553e-01],\n",
       "       [9.93724406e-01, 6.27559144e-03, 2.18654234e-10],\n",
       "       [4.10486665e-03, 9.93300676e-01, 2.59450777e-03],\n",
       "       [2.30606853e-10, 4.47720243e-03, 9.95522857e-01],\n",
       "       [5.87900956e-07, 1.43789858e-01, 8.56209576e-01],\n",
       "       [1.37770339e-03, 9.92205322e-01, 6.41697505e-03],\n",
       "       [1.25231509e-07, 1.21375203e-01, 8.78624618e-01],\n",
       "       [9.96497333e-01, 3.50273447e-03, 9.61306798e-11],\n",
       "       [9.97912228e-01, 2.08769995e-03, 1.34042985e-11],\n",
       "       [9.96917129e-01, 3.08285072e-03, 6.14986187e-11],\n",
       "       [9.98822868e-01, 1.17707253e-03, 3.92200291e-12],\n",
       "       [6.55721524e-05, 9.52137768e-01, 4.77965660e-02],\n",
       "       [1.24312713e-10, 3.17361951e-03, 9.96826410e-01],\n",
       "       [1.85296137e-03, 9.91685510e-01, 6.46156771e-03],\n",
       "       [3.59657977e-04, 9.78697121e-01, 2.09431835e-02],\n",
       "       [1.71767323e-10, 3.23334057e-03, 9.96766686e-01],\n",
       "       [9.96992469e-01, 3.00752837e-03, 7.17169588e-11],\n",
       "       [2.14857732e-06, 3.62284869e-01, 6.37713015e-01],\n",
       "       [9.95396674e-01, 4.60337196e-03, 1.58419430e-10],\n",
       "       [3.20877408e-10, 4.38817590e-03, 9.95611787e-01],\n",
       "       [4.41592540e-09, 9.93383303e-02, 9.00661707e-01],\n",
       "       [1.91972896e-10, 3.69743141e-03, 9.96302605e-01],\n",
       "       [5.87658977e-10, 7.76605727e-03, 9.92233992e-01],\n",
       "       [3.80684442e-11, 2.17686174e-03, 9.97823119e-01],\n",
       "       [9.93712723e-01, 6.28728885e-03, 5.52091151e-10],\n",
       "       [9.95709300e-01, 4.29066643e-03, 1.60351843e-10],\n",
       "       [9.98728693e-01, 1.27129490e-03, 2.98242550e-12],\n",
       "       [9.99465287e-01, 5.34771418e-04, 2.87206429e-13],\n",
       "       [1.02120335e-04, 9.76496994e-01, 2.34009083e-02],\n",
       "       [9.97899532e-01, 2.10041320e-03, 2.16089889e-11],\n",
       "       [9.97362316e-01, 2.63764570e-03, 4.36208708e-11],\n",
       "       [4.61728877e-09, 1.33489547e-02, 9.86651063e-01],\n",
       "       [1.10812973e-04, 9.71864879e-01, 2.80244164e-02],\n",
       "       [9.98077989e-01, 1.92202081e-03, 1.33646800e-11],\n",
       "       [9.98624921e-01, 1.37511815e-03, 5.34606040e-12],\n",
       "       [9.99497533e-01, 5.02446608e-04, 1.39143512e-13],\n",
       "       [5.00697013e-08, 4.20193933e-02, 9.57980633e-01],\n",
       "       [2.64300237e-04, 9.84670639e-01, 1.50650786e-02],\n",
       "       [2.67254236e-05, 9.13541138e-01, 8.64322037e-02],\n",
       "       [9.98773515e-01, 1.22651458e-03, 4.45946275e-12],\n",
       "       [9.98634398e-01, 1.36552728e-03, 4.24411981e-12],\n",
       "       [4.50918311e-03, 9.92476046e-01, 3.01471213e-03],\n",
       "       [5.58988586e-06, 6.15761638e-01, 3.84232819e-01],\n",
       "       [3.02877936e-07, 2.16190755e-01, 7.83808947e-01],\n",
       "       [8.42590744e-05, 9.63713646e-01, 3.62020880e-02],\n",
       "       [8.08104972e-12, 1.41740183e-03, 9.98582602e-01]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(scaled_X_test) # produces probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(scaled_X_test) # you will have predicted classes in orijinal form not as one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.argmax(axis=1) # we have the original classes back. from one-hot encoded to original class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0, 14,  1],\n",
       "       [ 0,  1, 15]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.93      0.93      0.93        15\n",
      "           2       0.94      0.94      0.94        16\n",
      "\n",
      "   micro avg       0.96      0.96      0.96        50\n",
      "   macro avg       0.96      0.96      0.96        50\n",
      "weighted avg       0.96      0.96      0.96        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test.argmax(axis=1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('myfirstmodel.h5') # if you rerun it will overwrite to avoid this after run comment out this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
